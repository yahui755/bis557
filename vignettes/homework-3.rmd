---
title: "Homework 3"
output: pdf_document
Imports: glmnet,Matrix


---

This homework is due by the end of the day on November 5th 2018. Solutions should appear as a vignette in your package called "homework-3". If you do not want to write your solution for questions 2 - 4 in \LaTeX, you may provide a solution using paper and pencil and include an image in your write-up. 

1. CASL page 117, question 7.
2. CASL page 200, question 3
3. CASL page 200, question 4
4. CASL page 200, question 5
5. CASL page 200, question 6

---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The ridge regression vignette}
-->

##1.
```{r}
set.seed(6909)
x <- rnorm(1000, 0, 1)
x.new <- sort(rnorm(50, 0, 1))
kernel_epan <- function(x) {
ran <- as.numeric(abs(x) <= 1)
val <- (3/4) * ( 1 - x^2 ) * ran
return(val)
}
kernel_epan(x)
i = 1
h = 1
kern_density <- function(x, h, x.new){
  dst_est <- numeric()
  for (i in 1:length(x.new)){
    dst_est[i] <- mean(kernel_epan((x.new[i]-x)/h))/h
  }
  dst_est
}
h = c(0.1, 0.5, 1, 2, 3)
for (i in h){
  plot(x.new, kern_density(x,i,x.new), ylab = "Density", main = "Kernal density estimation", type = "l")  
}
```


##2.
By definition of convex function, for $t \in [0, 1]$
$$
\begin{aligned}
f(t x + (1 - t)y) &\leq t f(x) + (1 - t)f(y)\\
g(t x + (1 - t)y) &\leq t g(x) + (1 - t)g(y)\\
\end{aligned}
$$
Then
$$
\begin{aligned}
h(t x + (1 - t)y) &= f(t x + (1 - t)y) + g(t x + (1 - t)y)\\
&\leq t f(x) + (1 - t)f(y) + t g(x) + (1 - t)g(y)\\
&=t (f(x) + g(x)) + (1 - t)(f(y) + g(y))\\
&=t h(x) + (1 - t) h(y)
\end{aligned}
$$
Therefore, $h = f + g$ is convex.

##3.
For $t \in [0, 1]$
$$
\begin{aligned}
f(x) &= |x|\\
f(tx+(1-t)y)&=|tx+(1-t)y|\\
&\leq |tx| + |(1-t)y|\\
&=f(tx)+f((1-t)y)\\
&=tf(x)+(1-t)f(y)
\end{aligned}
$$
Thus, $f(x)$ is convex.  
By Problem 3 (200.3), $l_1\ norm= \sum_{i = 1}|v_i|$ is convex

##4.

$l_2\ norm$
$$
\begin{aligned}
f(x) &= x^2\\
f(tx+(1-t)y) &=t^2x^2 + (1-t)^2y^2+2t(1-t)xy \\
tf(x)+(1-t)f(y)&=tx^2+(1-t)y\\
f(tx+(1-t)y)-[tf(x)+(1-t)f(y)]&=t(t-1)x^2+t(t-1)y^2+2t(1-t)xy\\
&=t(1-t)(-x^2-y^2+2xy)\\
&=t(t-1)(x-y)^2\\
&\leq0\ \ since\ 0\leq t \leq1\\
f(tx+(1-t)y) &\leq tf(x)+(1-t)f(y)
\end{aligned}
$$

Therefore, $f(x) = x^2$ is convex. 
Objective function of elastic net
$$\frac{1}{2n}||y-Xb||^2_2+\lambda[(1-\alpha)\frac{1}{2}||b||^2_2+\alpha||b||_1]$$
It's a sum of $l_2\ norm$ and $l_1\ norm$, by 3 and 4, it's convex

##5.
According to textbook page 189:
```{r}

#library(Matrix)

#library(glmnet)

# Check current KKT conditions for regression vector.
#Arguments:
# X: A numeric data matrix.
# y: Response vector.
# b: Current value of the regression vector.
# lambda: The penalty term.
#Return: 
#A logical vector indicating where the KKT conditions have been violated by the variables that are currently zero.
check_kkt <-
function(X, y, b, lambda)
{
resids <- y - X %*% b
s <- apply(X, 2, function(xj) crossprod(xj, resids)) /
lambda / nrow(X)
# Return a vector indicating where the KKT conditions have been violated by the variables that are currently zero.
(b == 0) & (abs(s) >= 1)
}
```
According to the textbook, we can generate a random dataset
```{r}
set.seed(365)
n <- 1000L
p <- 5000L
X <- matrix(rnorm(n * p), ncol = p)
beta <- c(seq(1, 0.1, length.out=(10L)), rep(0, p - 10L))
y <- X %*% beta + rnorm(n = n, sd = 0.15)
```
Use glmnet to implement lasso regression
```{r}

#Check which variables violate KKT condition
#x:A numeric data matrix.
#y: Response vector.
#Return: logical vector indicating where the KKT conditions have been violated
lasso_reg_with_screening <- function(x, y){
  fit <- cv.glmnet(X,y,alpha=1)
  lambda.1se <- fit$lambda.1se
  b <- fit$glmnet.fit$beta[, which(fit$lambda == fit$lambda.1se)]
  print(b)
  check_kkt(X, y, b, lambda.1se)
}
#We can observe that almost all coefficients get a false result for KKT violation. 
```
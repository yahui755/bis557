---
title: "Untitled"
output: html_document
---
###Question one
```{r setup, include=FALSE}
library(keras)
library(glmnet)
library(moments)
library(ggplot2)
library(readr)
library(tensorflow)
library(tibble)
```

```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
#Reshape the pixel matrix
x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

```


```{r}
set.seed(665)
s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t_1 <- table(as.vector(preds), y_test)
sum(diag(t_1)) / sum(t_1)
#prediction accuracy on test set: 0.8583
```

```{r}
set.seed(665)
#Exploration of features
#Credit to http://apapiu.github.io/2016-01-02-minst/
intensity<-apply(x_train,1,mean)
label<-aggregate(intensity,by=list(y_train),FUN = mean)
#From the plot, we can see the mean of pixels of different digits seem 
#to be quite different
plot<-ggplot(data =label,aes(x=Group.1,y=x))+geom_bar(stat = "identity",fill="yellow")
plot+scale_x_discrete(limits=0:9)+xlab("digit label")+ylab("average intensity")

kurtosis<-apply(x_train,1,kurtosis)
klabel<-aggregate(kurtosis,by=list(y_train),FUN = kurtosis)
#From the plot, we can see the kurtosis of pixels of different digits seem 
#to be quite different
plot<-ggplot(data = klabel,aes(x=Group.1,y=x))+geom_bar(stat = "identity",fill="green")
plot+scale_x_discrete(limits=0:9)+xlab("digit label")+ylab("kurtosis")
skewness<-apply(x_train,1,skewness)
slabel<-aggregate(skewness,by=list(y_train),FUN = skewness)
#From the plot, we can see the skewness of pixels of different digits seem 
#to be quite different
plot<-ggplot(data = slabel,aes(x=Group.1,y=x))+geom_bar(stat = "identity",fill="purple")
plot+scale_x_discrete(limits=0:9)+xlab("digit label")+ylab("skewness")
intensity<-as.vector(intensity)
x_train<-cbind(x_train,intensity)
x_test<-cbind(x_test,as.vector(apply(x_test,1,mean)))

#Model Performance
#With intensity in the features
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t_2 <- table(as.vector(preds), y_test)
sum(diag(t_2)) / sum(t_2)
#prediction accuracy on test set: 0.859, slightly better than the original model
```

```{r}
set.seed(665)
kurtosis<-as.vector(kurtosis)
x_train<-cbind(x_train,kurtosis)
x_test<-cbind(x_test,as.vector(apply(x_test,1,kurtosis)))
#With intensity and kurtosis in the features
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t_3 <- table(as.vector(preds), y_test)
sum(diag(t_3)) / sum(t_3)
#prediction accuracy on test set: 0.8595, slightly better than the original model
```

After adding the average intensity(mean) and the kurtosis of the pixels, the model performance become a little bit better in terms of prediction accuracy on test data. However, as the increase of accuracy is quite small, it may probably due to the choice of seed of our train set as I only 1000 samples selected from the train data. 

Also, we can try neural networks instead of LASSO to see if higher accuracy detected. Below is example of CNN.
```{r,eval=FALSE}
# This code was taken from 
# https://keras.rstudio.com/articles/examples/mnist_cnn.html
library(keras)
# Data Preparation -----------------------------------------------------
batch_size <- 128
num_classes <- 10
epochs <- 2
# Input image dimensions
img_rows <- 28
img_cols <- 28
# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define Model -----------------------------------------------------------
# Define model
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = input_shape) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)
# Train model
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2
)
scores <- model %>% evaluate(
  x_test, y_test, verbose = 0
)
# Output metrics
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')
```
The accuracy of prediction of CNN on the test dataset(>85%) is higher than LASSO. That's probably because I didn't use the whole data to train LASSO model. Or, CNN did perform better than LASSO. More experiments are needed to verify. 
###Question two

Adjust the kernel size, and any other parameters you think are useful, in the convolutional neural network for EMNIST in Section 8.10.4. Can you improve on the classification rate?  
```{r}
load("/Users/yahui/Desktop/emnist.Rdata")
# train set
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
  x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, num_classes = 26)
# test set
X_valid <- emnist$dataset[[2]][[1]]/255
x_valid <- array(dim = c(nrow(X_valid), 28, 28))
for (i in 1:nrow(X_valid)) {
  x_valid[i,,] <- matrix(X_valid[i,], nrow=28, ncol=28)
}
y_valid <- as.vector(emnist$dataset[[2]][[2]])-1
y_valid <- to_categorical(y_valid)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_valid <- array_reshape(x_valid, c(nrow(x_valid), 28, 28, 1))
```


### Modified Model

```{r, eval=FALSE}
model_2 <- keras_model_sequential()
model_2 %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                input_shape = c(28, 28, 1),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.4) %>%
  
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")
model_2 %>% compile(loss = "categorical_crossentropy",
                    optimizer = optimizer_rmsprop(),
                    metrics = c("accuracy"))
history_2 <- model_2 %>%
  fit(x_train, y_train, epochs = 10,
      validation_data = list(x_valid, y_valid))
```

```{r, eval=FALSE}
predict_train_2 = predict_classes(model_2, x_train)
train_prediction_acc_2 = mean(predict_train_2 == y_train_cat)
train_prediction_acc_2
```

```{r, eval=FALSE}
predict_valid_2 = predict_classes(model_2, x_valid)
valid_prediction_acc_2 = mean(predict_valid_2 == y_valid_cat)
valid_prediction_acc_2
```

![](model2_plot.png)

![](model2_results.png)

Both the in-sample classification rate and the out-of-sample classification rate have improved. 


###Question three
Edit code from P210.
```{r}
# Create list of weights to describe a dense neural network.
# Args: sizes: A vector giving the size of each layer, including the input and output layers.
# Returns: A list containing initialized weights and biases.
casl_nn_make_weights <- function(sizes) {
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L)) {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]), ncol = sizes[j], nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w, b=rnorm(sizes[j + 1L]))
  }
  weights 
}
# Apply a rectified linear unit (ReLU) to a vector/matrix.
# Args: v: A numeric vector or matrix.
# Returns: The original input with negative values truncated to zero.
casl_util_ReLU <- function(v) {
  v[v < 0] <- 0
  v
}
# Apply derivative of the rectified linear unit (ReLU).
# Args: v: A numeric vector or matrix.
# Returns: Sets positive values to 1 and negative values to zero.
casl_util_ReLU_p <- function(v) {
  p <- v * 0
  p[v > 0] <- 1
  p
}
```
Differentiate the loss function. Here use mean absolute deviation.
```{r}
# Derivative of the mean absolute deviation (MAD) function.
# Args: y: A numeric vector of responses.
#       a: A numeric vector of predicted responses.
# Returns: Returned current derivative the MAD function.
casl_util_mad_p <- function(y, a) {
  derloss <- c()
  for (i in 1:length(a)) {
    if (a[i] >= mean(y)) derloss[i]=1
    else derloss[i]=-1
  }
  return(derloss)
}
# Apply forward propagation to a set of NN weights and biases.
# Args: x: A numeric vector representing one row of the input.
#       weights: A list created by casl_nn_make_weights.
#       sigma: The activation function.
# Returns: A list containing the new weighted responses (z) and activations (a).
casl_nn_forward_prop <- function(x, weights, sigma) {
  L <- length(weights)
  z <- vector("list", L)
  a <- vector("list", L)
  for (j in seq_len(L)) {
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
    a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}
# Apply backward propagation algorithm.
# Args: x: A numeric vector representing one row of the input.
#       y: A numeric vector representing one row of the response.
#       weights: A list created by casl_nn_make_weights.
#       f_obj: Output of the function casl_nn_forward_prop.
#       sigma_p: Derivative of the activation function.
#       f_p: Derivative of the loss function.
# Returns: A list containing the new weighted responses (z) and activations (a).
casl_nn_backward_prop <- function(x, y, weights, f_obj, sigma_p, f_p) {
  z <- f_obj$z
  a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L))) {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]])
      } 
    else {
      grad_z[[j]] <- (t(weights[[j + 1]]$w) %*% grad_z[[j + 1]]) * sigma_p(z[[j]])
      }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}
# Apply stochastic gradient descent (SGD) to estimate NN.
# Args: X: A numeric data matrix.
#       y: A numeric vector of responses.
#       sizes: A numeric vector giving the sizes of layers in the neural network.
#       epochs: Integer number of epochs to computer.
#       eta: Positive numeric learning rate.
#       weights: Optional list of starting weights.
# Returns: A list containing the trained weights for the network.
casl_nn_sgd <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
    }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights, f_obj, casl_util_ReLU_p, casl_util_mad_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    } 
  }
  weights 
}
# Predict values from a training neural network.
# Args: weights: List of weights describing the neural network.
#       X_test: A numeric data matrix for the predictions.
# Returns: A matrix of predicted values.
casl_nn_predict <- function(weights, X_test) {
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
  }
  y_hat 
}
```
Simulation:
```{r}
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
yn <- X[,1,drop = FALSE]^ 2 + rnorm(1000, sd = 0.1)
ind <- sample(seq_along(yn), 100)
yn[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))
weights = casl_nn_sgd(X, yn, sizes = c(1, 25, 1), epochs=10, eta=0.001)
y_pred = casl_nn_predict(weights, X)
## visualiza the true value and predicted value
df = tibble(x = as.vector(X), y_pred = as.vector(y_pred),
             y = X[,1]^2, yn = as.vector(yn))
ggplot(df) + 
  geom_point(aes(x = x, y = yn)) +
  geom_line(aes(x = x, y = y_pred), color="blue") +
  labs(x = "x", y = "y (True vs. Predict)") + 
  ggtitle("Ture vs. Predicted Values with MSE")
```

From the two plots, it is clear that neural networks and SGD using mean absolute deviation as the loss function outperforms when using mean squared error as the loss function, which is expected. We conclude that when the data set contains outliers, the mean absolute deviation function is more robust compared to the mean squared error function in neural networks and SGD

